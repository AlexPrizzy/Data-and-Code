---
output:
  word_document: default
  html_document: default
  pdf_document: default
---
title: Jan 19th in class problems
author: Alex Przybycin

With the data loaded, answer the following questions. The objective here is twofold: 1) to practice your statistical computing skills, and 2) apply and explore error from fitting models on different sets of data.

1. Using some of the techniques we covered last week:

    a. Select only the Obama feeling thermometer (`ftobama`), household income (`faminc`), party affiliation on a 3 point scale (`pid3`), birth year (`birthyr`), and gender (`gender`) (*be sure to recode missing values to `NA` and omit these*)
  
    b. Split the subset data into training (75%) and testing (25%) sets (*hint*: remember to set the seed (`set.seed()`) prior to creating the split, as the proportions are generated at random)
  
    c. Plot the distributions of each against each other to ensure they look similar


library(here)
library(tidyverse)
NESdta <- read_csv(here("data", "anes_2016.csv"))

NESdta_short <- NESdta %>% 
  select(ftobama, pid3, birthyr, gender, faminc)
head(NESdta_short,
     n = 5)


set.seed(1234)

split_tidy <- initial_split(NESdta_short,
                            prop = 0.75)

train_tidy <- training(split_tidy)
test_tidy  <- testing(split_tidy)



tidy <- train_tidy %>% 
  ggplot(aes(x = ftobama)) + 
  geom_line(stat = "density") + 
  geom_line(data = test_tidy, 
            stat = "density", 
            col = "red") +
  labs(title = "Via Tidymodels") +
  theme_minimal()
```


2. Fit a linear regression (`lm()`) on the *training* data, predicting trump approval as a function of all other features.



training_regression <- lm( ftobama ~ pid3+birthyr+gender+faminc, data = train_tidy)



3. Calculate the training mean squared error (*hint*: consider using the `mse()` function from Dr. Soltoff's `rcfss` package, which is at the uc-cfss github, *not* on CRAN).


library(rcfss)

mse(train_tidy)



4. Calculate predictions for the testing set, using the model you built on the training set (*hint*: consider either `predict()` from base R, or `augment()` from `broom`).


library(base)

test_predict = predict(train_tidy)



5. Calculate the testing mean squared error.



training_mse = mean(train_tidy$residuals^2)




6. Compare the mean squared error from both sets numerically, side-by-side. What do you see? *Discuss in your groups and record a few sentences as a response.*

I'm not sure as to how to do this numerically. The function I used about gets the mean of the sum of squares from the training data.

7. Write your own function to calculate the MSE. Then, use it to re-answer questions 3 and 5. Present the results here, and compare with the `rcfss` approach via `mse()`. These results should be identical to the `mse()` version. Are they? If not, *why* do you think? (*just a sentence or two on your general thoughts if they differ*) 

I couldn't find the mse() function on the cfss github. Am I supposed to install that as a package in R first and find the documentation for it through R? 